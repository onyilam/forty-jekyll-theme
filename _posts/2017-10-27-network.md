---
layout: post
title: Uniform or not? 
description: My first foray into neural network and image classification 
image: assets/images/military.jpeg
---


The motivation of this project comes from wanting to understand whether certain media outlets are more likely to post pictures of individuals in uniform (firefighter, military or police). Posting pictures of individuals in uniform can suggest a narrative that resonates or sympathizes with the authority. I use the tool to classify the images posted on the instagram accounts of ABC (@abc) CNN (@cnn) and Fox News (@foxnews). The training sample consists of pictures scraped from the instagram accounts of US Army (@usarmy), US Police (@uspolice), and a generic search of pictures from Flickr. To improve mdoel accuracy, I also supplement the not-uniform class with pictures from ESPN (@espn) to help distinguis sports uniform from government uniforms. 

This is the first neural network I trained using 2048 images of individuals in uniform, and 1840 of random images for the training set. The test set has 464 uniform pictures, and 544 non-uniform pictures. The model achieves 88% accuracy, which is not great for production but decent on my first foray into neural networks. 

<img src="https://onyilam.github.io/assets/images/model_performance.png" width="700">


I collected 158 images from @abcnews, 473 from @cnn, and 1669 from @fox on October 2017, so many images . The model finds that 39%, 38%, and 44% of their images are considered as poeple in uniform. 


Couple things I learned:


1. The model has trouble separating individuals wearing suits vs individuals wearing uniforms. So many politicians are misclassified as people in uniform. To address this issue, I should have put more politicans pictures in the not-uniform folder to help train the model.

2. The model only works if the number of images in each class equals to the multiple of the batch size. For example, if the batch size is 16, you need to have 16 * X (X being any integer greater than 0) in order to probably feed the data into the model.

3. In order to predict the class of unseen images, we need to load the model twice. The first is the InceptionV3 architecture, and the second model is our actual trained model based on InceptionV3, which only contains the top layers.


<img src="https://onyilam.github.io/assets/images/test_model.png" width="200">


I follow  <a href = "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html">this tutorial </a> for the most part on binary classification for the model building. It is a very well written tutorial with My approach relies on leveraging a network pre-trained on a large dataset. Specifically, I employed the InceptionV3 architecture pre-trained on the ImageNet dataset. The original tutorial suggested to use the VGG16 Architecture which I found did not perform very well. And Curiously, the model performs worse when I tried to fine-tune the last convolutional block, so I stick with the bottleneck bottle which requires me to only train the top layers of the architecture.

To see code:

<ul class="actions">
					<li><a href="https://onyilam.github.io/" class="button">Github Repo</a></li>
			</ul>

